{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber\n",
    "\n",
    "def extract_pdf_text(pdf_url):\n",
    "    # Downloading PDF\n",
    "    response = requests.get(pdf_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    tekst = \"\"\n",
    "    with pdfplumber.open(io.BytesIO(response.content)) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                tekst += page_text + \"\\n\"\n",
    "    return tekst\n",
    "\n",
    "def extract_summary(tekst):\n",
    "    # Searching for 'Summary' and capturing everything that follows\n",
    "    if \"Sažetak\" in tekst:\n",
    "        return tekst.split(\"Sažetak\", 1)[1].strip()\n",
    "    return None\n",
    "\n",
    "def clean_article_links(article_links):\n",
    "    cleaned_links = []\n",
    "    for link in article_links:\n",
    "        # Check if the link contains '/view/'\n",
    "        if '/view/' in link:\n",
    "            view_index = link.index('/view/') + len('/view/')\n",
    "            cleaned_link = link[:view_index]  # Start with the base up to /view/\n",
    "            \n",
    "            # Loop through the characters after '/view/'\n",
    "            for char in link[view_index:]:\n",
    "                if char.isdigit():\n",
    "                    cleaned_link += char  # Append digits\n",
    "                elif char == '/':\n",
    "                    break  # Stop if we find a slash\n",
    "            \n",
    "            cleaned_links.append(cleaned_link)  # Append cleaned link\n",
    "        else:\n",
    "            cleaned_links.append(link)  # Keep it the same if it doesn't have /view/\n",
    "    return cleaned_links\n",
    "\n",
    "def get_article_links(archive_url):\n",
    "    links = set()\n",
    "    page = requests.get(archive_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # Extract links for each issue in the archive\n",
    "    issue_links = [a['href'] for a in soup.find_all('a', href=True) if 'issue/view' in a['href']]\n",
    "    \n",
    "    # Iterate over each issue to get article links\n",
    "    for issue_url in issue_links:\n",
    "        issue_page = requests.get(issue_url)\n",
    "        issue_soup = BeautifulSoup(issue_page.text, 'html.parser')\n",
    "        \n",
    "        # Find all article links within the issue page\n",
    "        article_links = [a['href'] for a in issue_soup.find_all('a', href=True) if 'article/view' in a['href']]\n",
    "        links.update(article_links)\n",
    "    \n",
    "    return list(links)\n",
    "\n",
    "archive_url = \"https://www.pregled.unsa.ba/index.php/pregled/issue/archive\"\n",
    "article_links = get_article_links(archive_url)\n",
    "print(\"Original Links:\", article_links)\n",
    "\n",
    "# Clean the links\n",
    "article_links = clean_article_links(article_links)\n",
    "print(\"Cleaned Links:\", article_links)  # Data collection list\n",
    "all_data = []\n",
    "\n",
    "for url in article_links:\n",
    "    # Fetching page content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parsing HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Data extraction\n",
    "    autori = [meta['content'] for meta in soup.find_all('meta', attrs={'name': 'DC.Creator.PersonalName'})]\n",
    "\n",
    "    datum_izdanja = soup.find('meta', attrs={'name': 'citation_date'})['content']\n",
    "    godina = datum_izdanja.split('/')[0]\n",
    "\n",
    "    naslov = soup.find('meta', attrs={'name': 'DC.Title'})['content']\n",
    "\n",
    "    stranice = soup.find('meta', attrs={'name': 'DC.Identifier.pageNumber'})['content']\n",
    "\n",
    "    publikacija = soup.find('meta', attrs={'name': 'DC.Source'})['content']\n",
    "\n",
    "    # URL extraction for PDF\n",
    "    pdf_url = soup.find('meta', attrs={'name': 'citation_pdf_url'})['content']\n",
    "\n",
    "    # Text extraction for PDF\n",
    "    tekst = extract_pdf_text(pdf_url)\n",
    "    procisceni = extract_summary(tekst)\n",
    "\n",
    "    data = {\n",
    "        'Naslov teksta': naslov,\n",
    "        'Naslov publikacije': publikacija,\n",
    "        'Autori': '; '.join(autori),\n",
    "        'Godina': godina,\n",
    "        'Stranice': stranice,\n",
    "        'Tekst': procisceni\n",
    "    }\n",
    "\n",
    "    all_data.append(data)\n",
    "   \n",
    "df = pd.DataFrame(all_data)\n",
    "df.head(55)  # Writing all data to a CSV file\n",
    "with open('Metadata.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=all_data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_data)\n",
    "   \n",
    "# Writing all data to a JSON file\n",
    "df.to_json('PDFScrap.json', orient='records', lines=True, force_ascii=False)  # Writing all data into a EXCEL file\n",
    "df.to_excel('ExcelScrap.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
