{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber -q\n",
    "import csv\n",
    "import io\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber\n",
    "from requests.exceptions import ChunkedEncodingError, ConnectionError, Timeout\n",
    "\n",
    "def extract_pdf_text(pdf_url):\n",
    "    # Downloading PDF\n",
    "    response = requests.get(pdf_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    tekst = \"\"\n",
    "    with pdfplumber.open(io.BytesIO(response.content)) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                tekst += page_text + \"\\n\"\n",
    "    return tekst\n",
    "\n",
    "def extract_summary(tekst):\n",
    "    # Searching for \"Summary\" and capturing everything that follows\n",
    "    if \"Sažetak\" in tekst:\n",
    "        return tekst.split(\"Sažetak\", 1)[1].strip()\n",
    "    return tekst\n",
    "\n",
    "def clean_article_links(article_links):\n",
    "    cleaned_links = []\n",
    "    for link in article_links:\n",
    "        # Check if the link contains '/view/'\n",
    "        if '/view/' in link:\n",
    "            view_index = link.index('/view/') + len('/view/')\n",
    "            cleaned_link = link[:view_index]  # Start with the base up to /view/\n",
    "\n",
    "            # Loop through the characters after '/view/'\n",
    "            for char in link[view_index:]:\n",
    "                if char.isdigit():\n",
    "                    cleaned_link += char  # Append digits\n",
    "                elif char == '/':\n",
    "                    break  # Stop if we find a slash\n",
    "\n",
    "            cleaned_links.append(cleaned_link)  # Append cleaned link\n",
    "        else:\n",
    "            cleaned_links.append(link)  # Keep it the same if it doesn't have /view/\n",
    "\n",
    "    return cleaned_links\n",
    "\n",
    "def get_article_links(archive_url):\n",
    "    links = set()\n",
    "    page = requests.get(archive_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # Extract links for each issue in the archive\n",
    "    issue_links = [a['href'] for a in soup.find_all('a', href=True) if 'issue/view' in a['href']]\n",
    "\n",
    "    # Iterate over each issue to get article links\n",
    "    for issue_url in issue_links:\n",
    "        issue_page = requests.get(issue_url)\n",
    "        issue_soup = BeautifulSoup(issue_page.text, 'html.parser')\n",
    "\n",
    "        # Find all article links within the issue page\n",
    "        article_links = [a['href'] for a in issue_soup.find_all('a', href=True) if 'article/view' in a['href']]\n",
    "        links.update(article_links)\n",
    "\n",
    "    return list(links)\n",
    "\n",
    "# Function to handle retries\n",
    "def fetch_with_retries(url, retries=3, delay=5, timeout=10):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout, stream=True)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except (requests.exceptions.RequestException, ConnectionError, ChunkedEncodingError, Timeout) as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"Retrying... Attempt {attempt + 1} of {retries}\")\n",
    "                sleep(delay)\n",
    "            else:\n",
    "                print(\"Max retries reached. Skipping.\")\n",
    "                return None\n",
    "\n",
    "archive_url = \"https://www.pregled.unsa.ba/index.php/pregled/issue/archive\"\n",
    "article_links = get_article_links(archive_url)\n",
    "print(\"Original Links:\", article_links)\n",
    "\n",
    "# Clean the links\n",
    "article_links = clean_article_links(article_links)\n",
    "print(\"Cleaned Links:\", article_links)\n",
    "\n",
    "all_data = []\n",
    "processed_titles = set()\n",
    "\n",
    "for url in article_links:\n",
    "    # Fetching page content with retries\n",
    "    response = fetch_with_retries(url)\n",
    "    if response is None:\n",
    "        continue\n",
    "\n",
    "    html_content = b''  # Initialize an empty byte string to store content\n",
    "\n",
    "    # Stream the response in chunks\n",
    "    for chunk in response.iter_content(chunk_size=1024):\n",
    "        html_content += chunk\n",
    "\n",
    "    # Parsing HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Data extraction with error handling\n",
    "    autori = [meta['content'] for meta in soup.find_all('meta', attrs={'name': 'DC.Creator.PersonalName'})]\n",
    "\n",
    "    # Get the publication date if available\n",
    "    datum_izdanja_meta = soup.find('meta', attrs={'name': 'citation_date'})\n",
    "    datum_izdanja = datum_izdanja_meta['content'] if datum_izdanja_meta else ''\n",
    "    godina = datum_izdanja.split('/')[0] if datum_izdanja else ''\n",
    "\n",
    "    # Get the title if available\n",
    "    naslov_meta = soup.find('meta', attrs={'name': 'DC.Title'})\n",
    "    naslov = naslov_meta['content'] if naslov_meta else None\n",
    "    print(naslov)\n",
    "\n",
    "    # Check for duplicates\n",
    "    if naslov in processed_titles:\n",
    "        print(f\"Skipping duplicate: {naslov}\")\n",
    "        continue\n",
    "\n",
    "    # Mark this title as processed\n",
    "    processed_titles.add(naslov)\n",
    "\n",
    "    # Get the page number if available\n",
    "    stranice_meta = soup.find('meta', attrs={'name': 'DC.Identifier.pageNumber'})\n",
    "    stranice = stranice_meta['content'] if stranice_meta else None\n",
    "\n",
    "    # Get the publication name if available\n",
    "    publikacija_meta = soup.find('meta', attrs={'name': 'DC.Source'})\n",
    "    publikacija = publikacija_meta['content'] if publikacija_meta else None\n",
    "\n",
    "    # URL extraction for PDF\n",
    "    pdf_url_meta = soup.find('meta', attrs={'name': 'citation_pdf_url'})\n",
    "    pdf_url = pdf_url_meta['content'] if pdf_url_meta else None\n",
    "\n",
    "    if pdf_url == None:\n",
    "        continue\n",
    "\n",
    "    # Text extraction for PDF if URL exists\n",
    "    tekst = extract_pdf_text(pdf_url) if pdf_url else ''\n",
    "    procisceni = extract_summary(tekst)\n",
    "\n",
    "    # Append the data to the list\n",
    "    data = {\n",
    "        'Naslov teksta': naslov,\n",
    "        'Naslov publikacije': publikacija,\n",
    "        'Autori': '; '.join(autori),\n",
    "        'Godina': godina,\n",
    "        'Stranice': stranice,\n",
    "        'Sadržaj': procisceni\n",
    "    }\n",
    "\n",
    "    all_data.append(data)\n",
    "df = pd.DataFrame(all_data)\n",
    "df.head(251)  # Writing all data to a CSV file\n",
    "with open('Metadata.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=all_data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_data)\n",
    "# Writing all data to a JSON file\n",
    "df.to_json('PDFScrap.json', orient='records', lines=True, force_ascii=False)\n",
    "# Writing all data into an EXCEL file\n",
    "df.to_excel('ExcelScrap.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
